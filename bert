from transformers import BertTokenizer, BertModel
import torch

# Load FinBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
model = BertModel.from_pretrained('yiyanghkust/finbert-tone')

def get_finbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        # Tokenize and encode the text
        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
        # Get the model outputs
        with torch.no_grad():
            outputs = model(**inputs)
        # Get the mean of the embeddings for all tokens
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return embeddings

# Sample data
control_descriptions = ["Description of control A", "Description of control B", "Description of control C"]
embeddings = get_finbert_embeddings(control_descriptions)

# Print the embeddings
for idx, emb in enumerate(embeddings):
    print(f"Embedding for control {idx+1}: {emb}"













from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the fine-tuned model and tokenizer
fine_tuned_model_path = "./fine-tuned-flan-t5"
tokenizer = T5Tokenizer.from_pretrained(fine_tuned_model_path)
model = T5ForConditionalGeneration.from_pretrained(fine_tuned_model_path)

# Your 3-page summary (replace with actual text)
three_page_summary = """
[Your 3-page summary text goes here]
"""

# Function to split text into chunks
def split_into_chunks(text, max_length=512):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(" ".join(current_chunk)) > max_length:
            chunks.append(" ".join(current_chunk[:-1]))
            current_chunk = [word]
    
    chunks.append(" ".join(current_chunk))  # Add the last chunk
    return chunks

# Split the 3-page summary into chunks
chunks = split_into_chunks(three_page_summary, max_length=512)

# Generate summaries for each chunk
short_summaries = []
for chunk in chunks:
    input_text = "summarize: " + chunk
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    short_summaries.append(summary)

# Combine the chunk summaries into a final summary
final_summary = " ".join(short_summaries)

# Print the final summary
print("Final Compressed Summary:\n", final_summary)



Some weights of the model checkpoint at ../downloaded_models/finBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at ../downloaded_models/finBERT and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



mport pandas as pd

# Read the DataFrame from an Excel file
df = pd.read_excel('your_file.xlsx')

# Select the last 15 columns
last_15_columns = df.columns[-15:]

# Initialize an empty DataFrame to store the results
summary_df = pd.DataFrame(columns=['Category', 'Positive (1)', 'Negative (0)', 'Total'])

# Iterate through each of the last 15 columns
for category in last_15_columns:
    positive_count = (df[category] == 1).sum()
    negative_count = (df[category] == 0).sum()
    total_count = df[category].count()
    
    # Append the results to the summary DataFrame
    summary_df = summary_df.append({
        'Category': category,
        'Positive (1)': positive_count,
        'Negative (0)': negative_count,
        'Total': total_count
    }, ignore_index=True)

# Print the summary DataFrame
print(summary_df)

# Optionally, save the summary DataFrame to an Excel file
summary_df.to_excel('summary_file.xlsx', index=False)












from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load the fine-tuned model and tokenizer
fine_tuned_model_path = "./fine-tuned-flan-t5"
tokenizer = T5Tokenizer.from_pretrained(fine_tuned_model_path)
model = T5ForConditionalGeneration.from_pretrained(fine_tuned_model_path)

# Your 3-page summary (replace with actual text)
three_page_summary = """
[Your 3-page summary text goes here]
"""

# Function to split text into chunks
def split_into_chunks(text, max_length=512):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(" ".join(current_chunk)) > max_length:
            chunks.append(" ".join(current_chunk[:-1]))
            current_chunk = [word]
    
    chunks.append(" ".join(current_chunk))  # Add the last chunk
    return chunks

# Split the 3-page summary into chunks
chunks = split_into_chunks(three_page_summary, max_length=512)

# Function to highlight headings and important points
def highlight_text(text):
    # Example: Adding custom tags (e.g., <h1> for headings, <strong> for important points)
    highlighted_text = text.replace("Heading:", "<h1>Heading:</h1>").replace("Important:", "<strong>Important:</strong>")
    return highlighted_text

# Generate summaries for each chunk
short_summaries = []
for chunk in chunks:
    input_text = "summarize: " + chunk
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    highlighted_summary = highlight_text(summary)
    short_summaries.append(highlighted_summary)

# Combine the chunk summaries into a final summary
final_summary = " ".join(short_summaries)

# Print the final summary
print("Final Compressed Summary:\n", final_summary)









from transformers import T5Tokenizer, T5ForConditionalGeneration
import re

# Load the fine-tuned model and tokenizer
fine_tuned_model_path = "./fine-tuned-flan-t5"
tokenizer = T5Tokenizer.from_pretrained(fine_tuned_model_path)
model = T5ForConditionalGeneration.from_pretrained(fine_tuned_model_path)

# The combined 3-page summary (replace with the actual 3-page summary text)
three_page_summary = """
[Your 3-page summary text goes here]
"""

# Custom prompt to guide summarization
prompt = "Summarize the following text in bullet points highlighting the key points:"

# Function to split text into chunks
def split_into_chunks(text, max_length=512):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(" ".join(current_chunk)) > max_length:
            chunks.append(" ".join(current_chunk[:-1]))
            current_chunk = [word]
    
    chunks.append(" ".join(current_chunk))  # Add the last chunk
    return chunks

# Function to format the summary with bullet points and highlight key points
def format_summary_with_highlights(text):
    lines = text.split('. ')
    bullet_points = []
    for line in lines:
        if line:
            line = line.strip()
            bullet_points.append(f"- **{line.split(':')[0]}:** {line.split(':', 1)[1].strip()}" if ':' in line else f"- {line}")
    return '\n'.join(bullet_points)

# Split the 3-page summary into chunks
chunks = split_into_chunks(three_page_summary, max_length=512)

# Generate summaries for each chunk
short_summaries = []
for chunk in chunks:
    input_text = f"{prompt} {chunk}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(input_ids, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    formatted_summary = format_summary_with_highlights(summary)
    short_summaries.append(formatted_summary)

# Combine the chunk summaries into a final summary
final_summary = "\n".join(short_summaries)

# Print the final summary
print("Final Compressed Summary with Highlights:\n", final_summary)


