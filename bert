from transformers import BertTokenizer, BertModel
import torch

# Load FinBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
model = BertModel.from_pretrained('yiyanghkust/finbert-tone')

def get_finbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        # Tokenize and encode the text
        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
        # Get the model outputs
        with torch.no_grad():
            outputs = model(**inputs)
        # Get the mean of the embeddings for all tokens
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return embeddings

# Sample data
control_descriptions = ["Description of control A", "Description of control B", "Description of control C"]
embeddings = get_finbert_embeddings(control_descriptions)

# Print the embeddings
for idx, emb in enumerate(embeddings):
    print(f"Embedding for control {idx+1}: {emb}"


Some weights of the model checkpoint at ../downloaded_models/finBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at ../downloaded_models/finBERT and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
