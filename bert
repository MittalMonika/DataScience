from transformers import BertTokenizer, BertModel
import torch

# Load FinBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
model = BertModel.from_pretrained('yiyanghkust/finbert-tone')

def get_finbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        # Tokenize and encode the text
        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
        # Get the model outputs
        with torch.no_grad():
            outputs = model(**inputs)
        # Get the mean of the embeddings for all tokens
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return embeddings

# Sample data
control_descriptions = ["Description of control A", "Description of control B", "Description of control C"]
embeddings = get_finbert_embeddings(control_descriptions)

# Print the embeddings
for idx, emb in enumerate(embeddings):
    print(f"Embedding for control {idx+1}: {emb}"


Some weights of the model checkpoint at ../downloaded_models/finBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertModel were not initialized from the model checkpoint at ../downloaded_models/finBERT and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



mport pandas as pd

# Read the DataFrame from an Excel file
df = pd.read_excel('your_file.xlsx')

# Select the last 15 columns
last_15_columns = df.columns[-15:]

# Initialize an empty DataFrame to store the results
summary_df = pd.DataFrame(columns=['Category', 'Positive (1)', 'Negative (0)', 'Total'])

# Iterate through each of the last 15 columns
for category in last_15_columns:
    positive_count = (df[category] == 1).sum()
    negative_count = (df[category] == 0).sum()
    total_count = df[category].count()
    
    # Append the results to the summary DataFrame
    summary_df = summary_df.append({
        'Category': category,
        'Positive (1)': positive_count,
        'Negative (0)': negative_count,
        'Total': total_count
    }, ignore_index=True)

# Print the summary DataFrame
print(summary_df)

# Optionally, save the summary DataFrame to an Excel file
summary_df.to_excel('summary_file.xlsx', index=False)
