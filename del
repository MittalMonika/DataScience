
import fitz  # PyMuPDF
import pandas as pd

def DocumentReader(pdf_path):
    """
    Extracts text from a PDF document.
    
    Args:
    pdf_path (str): Path to the PDF document.
    
    Returns:
    str: Extracted text from the PDF.
    """
    # Open the PDF file
    document = fitz.open(pdf_path)
    text = ""
    
    # Extract text from each page
    for page_num in range(len(document)):
        page = document.load_page(page_num)
        text += page.get_text()
    
    return text

def ClauseMiningProcessor(text):
    """
    Processes the extracted text into a structured DataFrame.
    
    Args:
    text (str): Extracted text from the PDF.
    
    Returns:
    pd.DataFrame: DataFrame with processed text content.
    """
    # Split the text into clauses based on some delimiter, e.g., periods.
    # You can customize this part to fit your specific needs.
    clauses = text.split('.')
    
    # Create a DataFrame from the clauses
    df = pd.DataFrame(clauses, columns=['Clause'])
    
    # Optionally, clean the DataFrame (e.g., remove empty clauses)
    df['Clause'] = df['Clause'].str.strip()
    df = df[df['Clause'] != '']
    
    return df

# Example usage
pdf_path = 'path/to/your/large/document.pdf'
text = DocumentReader(pdf_path)
df = ClauseMiningProcessor(text)

print(df.head())


      !pip install PyMuPDF pandas





from transformers import BertTokenizer, BertModel
import torch

# Load FinBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
model = BertModel.from_pretrained('yiyanghkust/finbert-tone')

def get_finbert_embeddings(text_list):
    embeddings = []
    for text in text_list:
        # Tokenize and encode the text
        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
        # Get the model outputs
        with torch.no_grad():
            outputs = model(**inputs)
        # Get the mean of the embeddings for all tokens
        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
    return embeddings

# Sample data
control_descriptions = ["Description of control A", "Description of control B", "Description of control C"]
embeddings = get_finbert_embeddings(control_descriptions)

# Print the embeddings
for idx, emb in enumerate(embeddings):
    print(f"Embedding for control {idx+1}: {emb}")


{
  "role": "system",
  "content": """
As an experienced legal analyst, create a brief summary focused on regulatory findings.
Along with the summary, answer the following questions and only add information if present, else state 'None'. 
DO NOT modify the text. Provide the output in JSON format.
What regulatory requirement was cited by the regulator, and how does it relate to the institution's operations? 
Where (country) did the event occur? 
Which regulator issued the finding?
Which businesses are impacted?
Does the document indicate what should have been done?
Was any reference made to a new or revised law or regulation?
Provide a concise summary in a paragraph without mentioning any other findings or actions needed.
"""
}



message = [
    {"role":"system","content":f"""
As an experienced legal analyst, create a brief summary in context of Regulatory with the findings.
Along with summary answer the following questions and only add information if present else says None 
DONOT modify the text.  Provide output in json format
What was cited by the regulator and its defination ? 
Where (country) the event occur? 
which regulator issued the finding?
which buisness are impacted?
Does the document indicate what we should have done?
Was any refernce made to a new or revised law or regulation ?
What was cited by the regulator and what was its defination ? 
Donot mention any other finding or action needed and just be concise and a paragraph 
"""},       
        {"role":"user","content":results['document_text']
               }]

deployment_id = "gpt-4-turbo-2024-04-09"

data  = completion_using_openai_sdk(deployment_id, message,max_tokens = 2100)

'{\n  "summary": "The FDIC conducted a two-week examination of a bank\'s compliance with 12 C.F.R. Part 370, focusing on the bank\'s IT system\'s ability to accurately calculate and report deposit insurance for each account in the event of bank failure. The examination revealed non-compliance with several provisions of the rule, although no enforcement action was required. The bank must respond formally by July 29 and has begun enhancing processes and controls.",\n  "What was cited by the regulator and its definition?": "12 C.F.R. Part 370, which requires covered institutions to configure IT systems to accurately calculate and report deposit insurance for each deposit account in a timely manner in the event of the bank\'s failure.",\n  "Where (country) the event occur?": "None",\n  "Which regulator issued the finding?": "FDIC",\n  "Which business are impacted?": "Business Banking, Consumer Banking",\n  "Does the document indicate what we should have done?": "None",\n  "Was any reference made to a new or revised law or regulation?": "None"\n}'
 Experimenting/exploring new technologies and methodologies related to Gen AI (LLM) and laying the playground for team members ensuring we are all equipped to innovate and succeed together


from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming clean_text function is defined and imported
df['process_description'] = df['process_description'].apply(clean_text)
df['risk_type_description'] = df['risk_type_description'].apply(clean_text)
df['control_description'] = df['control_description'].apply(clean_text)

# Vectorizing text
vectorizer = TfidfVectorizer(max_features=100)
features = vectorizer.fit_transform(df['process_description'] + ' ' + df['risk_type_description'] + ' ' + df['control_description']).toarray()


import networkx as nx

G = nx.Graph()
for index, row in df.iterrows():
    process_node = f"Process_{row['process_id']}"
    risk_node = f"Risk_{row['risk_id']}"
    control_node = f"Control_{row['control_id']}"

    G.add_node(process_node, type='process', features=features[index])
    G.add_node(risk_node, type='risk', features=features[index])
    G.add_node(control_node, type='control', features=features[index])

    G.add_edge(process_node, risk_node)
    G.add_edge(risk_node, control_node)


import torch
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GNNModel(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(GNNModel, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return x

model = GNNModel(input_dim=100, hidden_dim=64)  # Assume feature dimension is 100


optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.BCEWithLogitsLoss()

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    out = model(data)
    loss = criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()



model.eval()
with torch.no_grad():
    embeddings = model(data)
    similarity_matrix = cosine_similarity(embeddings.numpy())
    # Analyze similarities to suggest missing controls



import torch
from torch_geometric.data import Data

# Example of initializing node indices for unique entities
process_indices = {pid: idx for idx, pid in enumerate(sorted(df['process_id'].unique()))}
risk_indices = {rid: idx + len(process_indices) for idx, rid in enumerate(sorted(df['risk_id'].unique()))}
control_indices = {cid: idx + len(process_indices) + len(risk_indices) for idx, cid in enumerate(sorted(df['control_id'].unique()))}

# Preparing the edge indices
edge_sources = []
edge_targets = []

for _, row in df.iterrows():
    # Get indices from mappings
    process_idx = process_indices[row['process_id']]
    risk_idx = risk_indices[row['risk_id']]
    control_idx = control_indices[row['control_id']]
    
    # Connecting processes to risks (directional: process -> risk)
    edge_sources.append(process_idx)
    edge_targets.append(risk_idx)
    
    # Connecting risks to controls (directional: risk -> control)
    edge_sources.append(risk_idx)
    edge_targets.append(control_idx)

# Creating the edge_index tensor for PyTorch Geometric
edge_index = torch.tensor([edge_sources, edge_targets], dtype=torch.long)

# Assuming 'features' is a tensor with node features for all nodes
graph_data = Data(x=features, edge_index=edge_index)




 have process_id, process_name, process_description, risk id, risk type description, buisness specific risk , control_id, control name and ontrol description. i need to find missing control with in dataset relying on the logic similar processes should have similar controls.  process_id,risk id,control_id,  are number. they are meaningless but the number provided to specific things 
