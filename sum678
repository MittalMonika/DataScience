import re
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Define the text
text = """
The new regulation in the United States requires all financial institutions to comply with XYZ standards.
This regulation aims to enhance transparency and accountability within the financial sector.
Non-compliance with these standards may result in severe penalties and legal actions.
Financial institutions must implement the necessary measures to ensure full compliance with the new regulation.
The European Union has also introduced similar regulations recently.

* Tried the kafka-flink interface using docker
    * worked fine
    * kafka working (without PV)
* REFIT not deployed
* Tried to deploy kafka with PV mounted to the pods,
    * did not work, similar errors like in the REFIT.
* Times-Net: This again seems to be forecasting only the multi-variate time series data,
    * does not take into account the static and exogenous variables
    * does not take care of the target variable [not sure if that is feasible with this model]
"""

# Define keywords
keywords = ["regulation", "compliance", "standards", "penalties", "legal actions"]

# Preprocess text by identifying relevant paragraphs
def extract_paragraphs_with_keywords(text, keywords):
    paragraphs = text.split("\n")
    relevant_paragraphs = [para for para in paragraphs if any(keyword in para.lower() for keyword in keywords)]
    return relevant_paragraphs

relevant_paragraphs = extract_paragraphs_with_keywords(text, keywords)
preprocessed_text = "\n".join(relevant_paragraphs) + "\nKeywords: " + ", ".join(keywords)

# Load the Flan-T5 model and tokenizer
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")

def summarize_text(text, max_length=150, min_length=30, length_penalty=2.0, num_beams=4):
    # Preprocess the text for summarization
    input_text = "summarize: " + text
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

    # Generate the summary
    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=length_penalty, num_beams=num_beams, early_stopping=True)
    
    # Decode the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Summarize the preprocessed text
summary = summarize_text(preprocessed_text)
print(summary)




# Example text with regulatory and bullet points structure
text = """
Regulatory Section:
- This is a bullet point under the regulatory section.
- Another bullet point.
This paragraph is also part of the regulatory section.

Non-regulatory Section:
This paragraph does not start with a regulatory term.

Regulatory Section 2:
- Bullet point under another regulatory section.
- Another bullet point.
"""

def extract_relevant_sections(text, keywords):
    paragraphs = text.split("\n\n")  # Split text by double newline assuming paragraphs are separated
    relevant_sections = []
    current_section = []

    for paragraph in paragraphs:
        # Check if the paragraph starts with any of the keywords in lowercase
        if any(keyword in paragraph.lower() for keyword in keywords):
            # Add current section to relevant_sections if it's not empty
            if current_section:
                relevant_sections.append("\n".join(current_section))
                current_section = []  # Reset current_section

            # Start a new section with the current paragraph
            current_section.append(paragraph.strip())
        else:
            # Continue adding to the current section if not a new regulatory paragraph
            current_section.append(paragraph.strip())

    # Add the last section if it's not empty
    if current_section:
        relevant_sections.append("\n".join(current_section))

    return relevant_sections

# Keywords to identify regulatory sections
keywords = ["regulatory"]

# Extract relevant sections
relevant_sections = extract_relevant_sections(text, keywords)

# Print each relevant section separately
for i, section in enumerate(relevant_sections):
    print(f"Section {i + 1}:")
    print(section)
    print("-------------------")

pattern = rf'(^|\s)({re.escape(pattern)})([' + re.escape(''.join(punctuation_list)) + r']|\s|$)'
