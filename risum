with open_workbook('path_to_your_xlsb_file.xlsb') as wb:
    with wb.get_sheet(1) as sheet:  # Assuming the data is on the first sheet
        data = []
        for row in sheet.rows():
            data.append([item.v for item in row])

df = pd.DataFrame(data[1:], columns=data[0])


df['text'] = df.apply(lambda row: ' '.join([str(row[col]) for col in df.columns]), axis=1)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Example summarized text
summarized_text = "This is the summary generated by Flan-T5."

# Combine summarized text and all text from the report data for TF-IDF calculation
all_text = [summarized_text] + df['text'].tolist()

# Tokenize text
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(all_text)

# Calculate cosine similarity between summarized text and each section
similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])

# Find the index of the most relevant section
most_relevant_index = similarity_scores.argmax()

# Get the most relevant section from the report data
most_relevant_section = df.iloc[most_relevant_index - 1]




# Add similarity scores to the DataFrame
df['similarity_score'] = similarity_scores

# Sort the DataFrame based on similarity scores in descending order
sorted_df = df.sort_values(by='similarity_score', ascending=False)

# Print the top 3 matches
print("Top 3 Matches:")
for index, row in sorted_df.head(3).iterrows():
    print(row)


print("Most Relevant Section:")
print(most_relevant_section)




We have a large Excel report containing multiple fields, such as obligation name, description, ID, issuing authority, and more.
Our goal was to identify the most relevant sections of this report that match a given summary generated by Flan-T5, an advanced AI model for text summarization.
We calculated the similarity between our summary and the report's sections to find the best matches.


def custom_agg(series):
    unique_values = series.unique()
    if len(unique_values) == 1:
        return unique_values[0]
    else:
        return ', '.join(map(str, unique_values))

# Group by 'ID' and apply custom aggregation
merged_df = df.groupby('ID').agg(lambda x: custom_agg(x)).reset_index()

print(merged_df)



# Group by process_id and count unique risk_id and control_id
result = df.groupby('process_id').agg({
    'risk_id': 'nunique',
    'control_id': 'nunique'
}).reset_index()

# Rename columns for clarity
result.columns = ['process_id', 'unique_risk_count', 'unique_control_count']

print(result)





import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Example summarized text
summarized_text = "This is the summary generated by Flan-T5."

# Example DataFrame with multiple columns including 'business_specific_risk'
data = {
    'process_id': [1, 2, 3],
    'risk_id': [10, 12, 13],
    'control_id': [100, 102, 103],
    'business_specific_risk': [
        "Risk description 1.",
        "Risk description 2.",
        "Risk description 3."
    ]
}

df = pd.DataFrame(data)

# Combine summarized text and all business-specific risk text for TF-IDF calculation
all_text = [summarized_text] + df['business_specific_risk'].tolist()

# Tokenize text
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(all_text)

# Calculate cosine similarity between summarized text and each business-specific risk text
similarity_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])

# Flatten the similarity scores array
similarity_scores = similarity_scores.flatten()

# Add similarity scores to the DataFrame
df['similarity_score'] = similarity_scores

# Sort the DataFrame based on similarity scores in descending order
sorted_df = df.sort_values(by='similarity_score', ascending=False)

# Print the sorted DataFrame with similarity scores
print("Sorted DataFrame with Similarity Scores:")
print(sorted_df)

# Print the top 3 matches
print("\nTop 3 Matches:")
for index, row in sorted_df.head(3).iterrows():
    print(f"Process ID: {row['process_id']}, Risk ID: {row['risk_id']}, Control ID: {row['control_id']}, Business-Specific Risk: {row['business_specific_risk']}, Similarity Score: {row['similarity_score']}")

# Get the most relevant section from the DataFrame
most_relevant_section = sorted_df.iloc[0]

print("\nMost Relevant Section:")
print(most_relevant_section)


def clean_text(text, punctuation_list):
    # Create regex pattern to match any of the punctuation marks in the list
    pattern = r'[' + re.escape(''.join(punctuation_list)) + r']'
    
    # Use regex to remove punctuation marks
    cleaned_text = re.sub(pattern, '', text)
    return cleaned_text

# Clean the example text using the list of punctuation marks
cleaned_text = clean_text(text, pun)


        pattern = rf'(^|\s)({re.escape(pattern)})([' + re.escape(''.join(punctuation_list)) + r']|\s|$)'


def clean_text(text, punctuation_list, words):
    # Create a regex pattern to match punctuation marks from the list that appear before or after specific words
    pattern = r'([' + re.escape(''.join(punctuation_list)) + r']?)(\s*)(' + '|'.join(re.escape(word) for word in words) + r')(\s*)([' + re.escape(''.join(punctuation_list)) + r']?)'
    
    # Use regex to remove punctuation marks that meet the condition
    cleaned_text = re.sub(pattern, r'\3', text)
    return cleaned_text


