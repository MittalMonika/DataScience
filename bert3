from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
from sentence_transformers import SentenceTransformer, util

# Load models
tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')
model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

def summarize_text(text, max_length=150, min_length=80, length_penalty=1.5, num_beams=6, early_stopping=True):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(
        inputs, 
        max_length=max_length, 
        min_length=min_length, 
        length_penalty=length_penalty, 
        num_beams=num_beams, 
        early_stopping=early_stopping
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

def remove_redundancy(sentences, threshold=0.8):
    embeddings = sentence_model.encode(sentences, convert_to_tensor=True)
    unique_sentences = []
    for i, sentence in enumerate(sentences):
        is_unique = True
        for j in range(i):
            if util.pytorch_cos_sim(embeddings[i], embeddings[j]) > threshold:
                is_unique = False
                break
        if is_unique:
            unique_sentences.append(sentence)
    return unique_sentences

def chunk_text(text, chunk_size=512, overlap=200):
    tokens = tokenizer.encode(text, return_tensors="pt")[0]
    chunks = []
    start = 0
    while start < len(tokens):
        end = min(start + chunk_size, len(tokens))
        chunk = tokens[start:end]
        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))
        if end == len(tokens):
            break
        start = end - overlap
    return chunks

def main(text, chunk_size=512, overlap=200):
    # Chunking the text with overlap
    chunks = chunk_text(text, chunk_size, overlap)
    
    # Summarize each chunk
    chunk_summaries = [summarize_text(chunk) for chunk in chunks]
    
    # Combine summaries
    combined_summary = " ".join(chunk_summaries)
    
    # Split into sentences
    sentences = combined_summary.split('. ')
    
    # Remove redundancy
    unique_sentences = remove_redundancy(sentences)
    
    # Join sentences to form the final summary
    final_summary = '. '.join(unique_sentences)
    
    # Refine the final summary
    refined_summary = summarize_text(final_summary, max_length=100, min_length=50)
    
    return refined_summary

# Example usage
text = """
Your full text goes here. Make sure it's clean and well-formatted.
"""

summary = main(text)
print(summary)
