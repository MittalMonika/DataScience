



mport numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Combine embeddings
X = np.stack(df['control_embeddings'].values)

# Standardize the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train a logistic regression model for each process
process_columns = [col for col in df.columns if col.startswith('BP_id')]

# Dictionary to store models for each process
models = {}
for process in process_columns:
    y = df[process]
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    models[process] = model

    # Predict and evaluate (optional, since we are using the entire dataset)
    y_pred = model.predict(X)
    print(f"Accuracy for {process}:", accuracy_score(y, y_pred))


# Calculate applicability scores for all controls for each process
for process in process_columns:
    df[f'{process}_applicability_score'] = models[process].predict_proba(X)[:, 1]

# Example to print out the applicability scores
applicability_scores_columns = [f'{process}_applicability_score' for process in process_columns]
print(df[['control_id'] + applicability_scores_columns])


from sklearn.metrics.pairwise import cosine_similarity

# Calculate cosine similarity between all control descriptions
control_embeddings = np.stack(df['control_embeddings'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Adding similarity score to DataFrame for visualization or further processing
df['similarity_scores'] = [similarity_matrix[i] for i in range(len(similarity_matrix))]


# Determine missing controls based on thresholds
threshold_applicability = 0.5
threshold_similarity = 0.7

def is_missing(row):
    return any(row[f'{bp}_applicability_score'] > threshold_applicability for bp in ['BP_1', 'BP_2', 'BP_3']) and \
           all(max(row['similarity_scores']) < threshold_similarity for bp in ['BP_1', 'BP_2', 'BP_3'])

df['is_missing'] = df.apply(is_missing, axis=1)

missing_controls = df[df['is_missing']]

print("Missing Controls:")
print(missing_controls[['control_id', 'control_name', 'BP_1_applicability_score', 'BP_2_applicability_score', 'BP_3_applicability_score', 'similarity_scores']])




def get_finbert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use the CLS token embeddings
    embeddings = outputs.last_hidden_state[:, 0, :]
    return embeddings.cpu().numpy().flatten()

# Get embeddings for control descriptions
df['control_embeddings'] = df['cleaned_control_description'].apply(lambda x: get_finbert_embeddings(x))






#-----test---
for process in process_columns:
    df[f'{process}_applicability_score'] = models[process].predict_proba(X)[:, 1]
# Reshape the DataFrame to long format
applicability_scores_columns = [f'{process}_applicability_score' for process in process_columns]

# Prepare the long format DataFrame
long_df = pd.melt(
    df,
    id_vars=['control_id'],
    value_vars=applicability_scores_columns,
    var_name='BP_name',
    value_name='BP_applicability_score'
)

# Extract the process name from the column name
long_df['BP_name'] = long_df['BP_name'].str.replace('_applicability_score', '')

# Print the reshaped DataFrame
print(long_df)

sorted_df = long_df.sort_values(by='BP_applicability_score', ascending=False)

# Format applicability score to two decimal places
sorted_df['BP_applicability_score'] = sorted_df['BP_applicability_score'].apply(lambda x: round(x, 2))



from sklearn.metrics.pairwise import cosine_similarity

# Calculate cosine similarity between control embeddings
control_embeddings = np.stack(df['control_embeddings'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Adding similarity score to DataFrame
df['similarity_scores'] = [similarity_matrix[i] for i in range(len(similarity_matrix))]

# Prepare a DataFrame to store the mappings (0 or 1)
mapped_data = {
    'control_id': [101, 102, 103, 104, 105],  # Example control IDs
    'BP_id1': [1, 0, 1, 0, 1],  # Example mappings (0 or 1)
    'BP_id2': [0, 1, 0, 1, 0],  # Example mappings (0 or 1)
    'BP_id3': [1, 1, 0, 0, 1],  # Example mappings (0 or 1)
    # Add more processes as needed
}

mapped_df = pd.DataFrame(mapped_data)

# Reshape the mappings DataFrame to long format
mapped_long_df = pd.melt(
    mapped_df,
    id_vars=['control_id'],
    var_name='BP_name',
    value_name='mapped'
)

# Combine the sorted DataFrame with the similarity scores and the mappings
merged_df = pd.merge(sorted_df, mapped_long_df, on=['control_id', 'BP_name'], how='left')

# Fill missing values in the 'mapped' column with 0
merged_df['mapped'] = merged_df['mapped'].fillna(0).astype(int)

# Print the merged DataFrame
print(merged_df)







from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

# Sample DataFrame with control_id, BP_id, and applicability_score
data = {
    'control_id': [101, 102, 103, 104, 105],
    'BP_id': ['BP1', 'BP2', 'BP1', 'BP3', 'BP2'],
    'applicability_score': [0.9, 0.85, 0.95, 0.88, 0.91],
    'control_embeddings': [
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768)
    ]
}

df = pd.DataFrame(data)

# Calculate cosine similarity between control embeddings
control_embeddings = np.stack(df['control_embeddings'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Iterate over each control to find similar controls
for i in range(len(df)):
    similar_controls = []
    for j in range(len(df)):
        if i != j:
            similarity_score = similarity_matrix[i][j]
            similar_controls.append({
                'similar_control_id': df.loc[j, 'control_id'],
                'similarity_score': similarity_score,
                'mapped': int(df.loc[j, 'BP_id'] == df.loc[i, 'BP_id'])
            })
    # Add each similar control as a new row to the DataFrame
    for similar_control in similar_controls:
        df = df.append({
            'control_id': df.loc[i, 'control_id'],
            'BP_id': df.loc[i, 'BP_id'],
            'applicability_score': df.loc[i, 'applicability_score'],
            'similar_control_id': similar_control['similar_control_id'],
            'similarity_score': similar_control['similarity_score'],
            'mapped': similar_control['mapped']
        }, ignore_index=True)

# Print the modified DataFrame
print(df[['control_id', 'BP_id', 'applicability_score', 'similar_control_id', 'similarity_score', 'mapped']])












from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

# Sample dataframes
df_sup_data = {
    'control_id': [101, 102, 103, 104, 105],
    'name_description': ['Desc 1', 'Desc 2', 'Desc 3', 'Desc 4', 'Desc 5'],
    'embedding': [
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768),
        np.random.rand(768)
    ],
    'Y': [[1, 0, 1], [0, 1, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]]
}
df_sup = pd.DataFrame(df_sup_data)

sorted_df_data = {
    'control_id': [101, 102, 103, 104, 105],
    'BP_id': ['BP1', 'BP2', 'BP1', 'BP3', 'BP2'],
    'applicability_score': [0.9, 0.85, 0.95, 0.88, 0.91]
}
sorted_df = pd.DataFrame(sorted_df_data)

# Calculate cosine similarity between control embeddings in df_sup
control_embeddings = np.stack(df_sup['embedding'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Function to find similar controls for a given control_id
def find_similar_controls(control_id):
    similar_controls = []
    idx = df_sup[df_sup['control_id'] == control_id].index[0]
    for i, score in enumerate(similarity_matrix[idx]):
        if df_sup.loc[i, 'control_id'] != control_id:
            similar_controls.append({
                'similar_control_id': df_sup.loc[i, 'control_id'],
                'similarity_score': score,
                'mapped': any(df_sup.loc[i, 'Y'])
            })
    return similar_controls

# Add similar controls to sorted_df
similar_controls_list = []
for control_id in sorted_df['control_id']:
    similar_controls = find_similar_controls(control_id)
    similar_controls_list.append(similar_controls)

# Flatten the list of similar controls
flattened_similar_controls = [item for sublist in similar_controls_list for item in sublist]

# Create a DataFrame from the flattened list
similar_controls_df = pd.DataFrame(flattened_similar_controls)

# Merge similar_controls_df with sorted_df
sorted_df = pd.merge(sorted_df, similar_controls_df, left_on='control_id', right_on='similar_control_id', how='left')

# Drop unnecessary columns and print the result
sorted_df = sorted_df.drop('similar_control_id', axis=1)
print(sorted_df)



def find_similar_controls(control_id):
    similar_controls = []
    idx = df_sup[df_sup['control_id'] == control_id].index[0]
    for i, score in enumerate(similarity_matrix[idx]):
        if df_sup.loc[i, 'control_id'] != control_id:
            mapped_columns = ['BP_1', 'BP_2']  # Add all BP column names here
            mapped_values = [df_sup.loc[i, col] for col in mapped_columns]
            similar_controls.append({
                'similar_control_id': df_sup.loc[i, 'control_id'],
                'similarity_score': score,
                'mapped': any(mapped_values)  # Check if any BP column is 1
            })
    return similar_controls






------


from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd

# Assuming 'embedding' column contains the embeddings as lists or arrays
control_embeddings = np.stack(df_sup['embedding'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Initialize an empty list to store the data
merged_data = []

# Get the process columns (assumed to be named as 'BP_1', 'BP_2', ..., 'BP_n')
process_columns = [col for col in df_sup.columns if col.startswith('BP_')]

# Iterate over the similarity matrix
for idx, control_id in enumerate(df_sup['control_id']):
    for i, score in enumerate(similarity_matrix[idx]):
        if df_sup.loc[i, 'control_id'] != control_id:
            # Get the similar control ID
            similar_control_id = df_sup.loc[i, 'control_id']
            
            # Check if the original control and the similar control belong to the same process
            if not any(df_sup.loc[idx, process_columns] & df_sup.loc[i, process_columns]):
                # Check if the similar control is already mapped to the same process
                similar_control_mapped = int(any(df_sup.loc[i, process_columns] & df_sup.loc[idx, process_columns]))
                
                # Get the BP_id and applicability score for the original control
                BP_id = sorted_df.loc[sorted_df['control_id'] == control_id, 'BP_id'].values[0]
                applicability_score = sorted_df.loc[sorted_df['control_id'] == control_id, 'applicability_score'].values[0]
                
                # Add the data to the merged_data list
                merged_data.append({
                    'control_id': control_id,
                    'BP_id': BP_id,
                    'applicability_score': applicability_score,
                    'similar_control_id': similar_control_id,
                    'similarity_score': score,
                    'mapped': similar_control_mapped
                })

# Create the final DataFrame
final_df = pd.DataFrame(merged_data)

# Print the final DataFrame
print(final_df)





