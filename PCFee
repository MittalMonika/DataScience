



mport numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Combine embeddings
X = np.stack(df['control_embeddings'].values)

# Standardize the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train a logistic regression model for each process
process_columns = [col for col in df.columns if col.startswith('BP_id')]

# Dictionary to store models for each process
models = {}
for process in process_columns:
    y = df[process]
    model = LogisticRegression(max_iter=1000)
    model.fit(X, y)
    models[process] = model

    # Predict and evaluate (optional, since we are using the entire dataset)
    y_pred = model.predict(X)
    print(f"Accuracy for {process}:", accuracy_score(y, y_pred))


# Calculate applicability scores for all controls for each process
for process in process_columns:
    df[f'{process}_applicability_score'] = models[process].predict_proba(X)[:, 1]

# Example to print out the applicability scores
applicability_scores_columns = [f'{process}_applicability_score' for process in process_columns]
print(df[['control_id'] + applicability_scores_columns])


from sklearn.metrics.pairwise import cosine_similarity

# Calculate cosine similarity between all control descriptions
control_embeddings = np.stack(df['control_embeddings'].values)
similarity_matrix = cosine_similarity(control_embeddings)

# Adding similarity score to DataFrame for visualization or further processing
df['similarity_scores'] = [similarity_matrix[i] for i in range(len(similarity_matrix))]


# Determine missing controls based on thresholds
threshold_applicability = 0.5
threshold_similarity = 0.7

def is_missing(row):
    return any(row[f'{bp}_applicability_score'] > threshold_applicability for bp in ['BP_1', 'BP_2', 'BP_3']) and \
           all(max(row['similarity_scores']) < threshold_similarity for bp in ['BP_1', 'BP_2', 'BP_3'])

df['is_missing'] = df.apply(is_missing, axis=1)

missing_controls = df[df['is_missing']]

print("Missing Controls:")
print(missing_controls[['control_id', 'control_name', 'BP_1_applicability_score', 'BP_2_applicability_score', 'BP_3_applicability_score', 'similarity_scores']])




def get_finbert_embeddings(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use the CLS token embeddings
    embeddings = outputs.last_hidden_state[:, 0, :]
    return embeddings.cpu().numpy().flatten()

# Get embeddings for control descriptions
df['control_embeddings'] = df['cleaned_control_description'].apply(lambda x: get_finbert_embeddings(x))






#-----test---
for process in process_columns:
    df[f'{process}_applicability_score'] = models[process].predict_proba(X)[:, 1]
# Reshape the DataFrame to long format
applicability_scores_columns = [f'{process}_applicability_score' for process in process_columns]

# Prepare the long format DataFrame
long_df = pd.melt(
    df,
    id_vars=['control_id'],
    value_vars=applicability_scores_columns,
    var_name='BP_name',
    value_name='BP_applicability_score'
)

# Extract the process name from the column name
long_df['BP_name'] = long_df['BP_name'].str.replace('_applicability_score', '')

# Print the reshaped DataFrame
print(long_df)

sorted_df = long_df.sort_values(by='BP_applicability_score', ascending=False)

# Format applicability score to two decimal places
sorted_df['BP_applicability_score'] = sorted_df['BP_applicability_score'].apply(lambda x: round(x, 2))

