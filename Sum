
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Specify the model name for the smaller variant, which is more manageable for testing
model_name = "google/flan-t5-small"

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Move the model to GPU for faster processing
model = model.to("cuda")

print("Model and tokenizer are loaded and moved to GPU.")
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")

inputs = tokenizer("A step by step recipe to make bolognese pasta:", return_tensors="pt")

outputs = model.generate(**inputs)

print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
['Pour a cup of bolognese into a large bowl and add the pasta']


import torch

def summarize_text(text, model, tokenizer, max_length=150):
    # Prepend the prompt to the input text
    input_text = "summarize: " + text
    
    # Encode the text input to tensor
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)
    
    # Generate summary output using the model
    with torch.no_grad():
        summary_ids = model.generate(
            input_ids,
            max_length=max_length,
            num_beams=4,
            length_penalty=2.0,
            early_stopping=True
        )
    
    # Decode and return the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example text
sample_text = """
The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods. 
In 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.
"""

# Call the summarization function
summary = summarize_text(sample, model, tokenizer)
print("Original Text:", sample_text)
print("Summary:", summary)



import streamlit as st
from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline
import torch

# Model and tokenizer
model_checkpoint = "LaMini-Flan-T5-248M"
tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)
model = T5ForConditionalGeneration.from_pretrained(model_checkpoint, device_map='auto', torch_dtype=torch.float32)

# Summarization Pipeline Setup
def summarization_pipeline(text):
    summarizer = pipeline('summarization', model=model, tokenizer=tokenizer, max_length=500, min_length=70)
    return summarizer(text)

# Handle each chunk
def process_chunks(chunks):
    summaries = []
    for chunk in chunks:
        summary = summarization_from_pipeline(chunk)
        summaries.append(summary)
    return summaries

# Query Functionality
def query_summaries(query, summaries):
    results = [summary for summary in summaries if query.lower() in summary.lower()]
    return results

@st.cache_data
def display_pdf(file):
    # Existing PDF display code

def main():
    st.title("Document Summarization App using Language Model")
    uploaded_file = st.file_uploader("Upload your PDF file", type=['pdf'])

    if uploaded_file is not None:
        if st.button("Summarize"):
            col1, col2 = st.columns(2)
            filepath = "pdf/" + uploaded_file.name
            with open(filepath, "wb") as temp_file:
                temp_file.write(uploaded_file.read())
            with col1:
                st.info("Uploaded File")
                pdf_view = display_pdf(filepath)

            with col2:
                st.info("Processing...")
                chunks = chunk_text_from_file(filepath)  # Define this to split the PDF into manageable parts
                summaries = process_chunks(chunks)
                summarized_text = " ".join(summaries)  # Joining summaries for display

                st.info("Summarization Complete")
                st.success(summarized_text)

                query = st.text_input("Enter your query:")
                if st.button("Search"):
                    results = query_summaries(query, summaries)
                    st.write(results)

if __name__ == "__main__":
    main()






Summarize the key themes and insights from Jamie Dimon's Letter to Shareholders from the 2022 Annual Report of JPMorgan Chase & Co. Focus on the challenges and achievements of the year, the strategic direction of the bank, and the overarching values that guide its operations. Highlight the bank's response to global economic conditions, initiatives in corporate responsibility, and future outlook as discussed by Jamie Dimon.
