import pandas as pd

# Sample DataFrame
data = {
    'column1': ['regulation and compliance', 'finance report', 'investment law', 'loan details'],
    'column2': ['risk analysis', 'banking regulation', 'market trends', 'financial law'],
    'column3': ['summary of findings', 'report details', 'document review', 'law and policy'],
    'country': ['USA', 'UK', 'Canada', 'Australia'],
    'business_impact': ['High', 'Medium', 'Low', 'Medium']
}
df = pd.DataFrame(data)

# Keywords to search for
keywords = ['regulation', 'compliance', 'law']

# Additional filters
country_text = 'UK'
business_impact = 'Medium'

# Combine all columns into a single string for each row
combined_columns = df.apply(lambda x: ' '.join(x.astype(str).str.lower()), axis=1)

# Check if any keyword is in the combined string
keyword_pattern = '|'.join(keywords)
keyword_filtered_df = df[combined_columns.str.contains(keyword_pattern, na=False)]

# Filter based on country_text if provided, otherwise fill with None
if country_text:
    country_filtered_df = keyword_filtered_df[keyword_filtered_df['country'] == country_text]
else:
    country_filtered_df = keyword_filtered_df.copy()
    country_filtered_df['country'] = None

# Filter based on business_impact if provided
if business_impact:
    final_filtered_df = country_filtered_df[country_filtered_df['business_impact'] == business_impact]
else:
    final_filtered_df = country_filtered_df.copy()
    final_filtered_df['business_impact'] = None

print(final_filtered_df)




import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util

# Load the data into a DataFrame
df = pd.read_csv('path_to_your_file.csv')  # Adjust this line to load your data
text_column = 'text_column'  # Replace with the actual name of your text column

# Initialize the model
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Define batch size
batch_size = 1000

# Function to process and get embeddings in batches
def batch_encode_texts(texts, model, batch_size=32):
    embeddings = []
    for start in range(0, len(texts), batch_size):
        end = min(start + batch_size, len(texts))
        batch_texts = texts[start:end]
        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)
        embeddings.extend(batch_embeddings.cpu().numpy())
    return np.array(embeddings)

# Get embeddings for all texts in the DataFrame
embeddings = batch_encode_texts(df[text_column].tolist(), model, batch_size)

# Save the embeddings to a numpy file for faster access later
np.save('embeddings.npy', embeddings)

# Function to perform semantic search
def semantic_search(query, embeddings, model, df, top_k=5):
    query_embedding = model.encode(query, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_embedding, embeddings).numpy().flatten()
    top_results = np.argpartition(-cos_scores, range(top_k))[:top_k]
    return df.iloc[top_results]

# Example usage
query = "Sample query text"
top_k_results = semantic_search(query, embeddings, model, df, top_k=5)
print(top_k_results)

